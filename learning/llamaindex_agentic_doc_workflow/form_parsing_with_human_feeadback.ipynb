{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/form_parsing_with_human_feeadback.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex\n",
    "    , StorageContext\n",
    "    , load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent\n",
    "    , StopEvent\n",
    "    , Workflow\n",
    "    , step\n",
    "    , Event\n",
    "    , Context\n",
    "    , InputRequiredEvent\n",
    "    , HumanResponseEvent\n",
    ")\n",
    "from helper import get_llama_cloud_api_key, get_llama_cloud_base_url, extract_html_content\n",
    "from IPython.display import display, HTML\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cloud_api_key = get_llama_cloud_api_key()\n",
    "llama_cloud_base_url = get_llama_cloud_base_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "    field: str \n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    response: str\n",
    "\n",
    "class FeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class GenerateQuestionEvent(Event):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    storage_dir = \"./storage\"\n",
    "    llm: Ollama\n",
    "    query_engine: VectorStoreIndex\n",
    "    embed_model: OllamaEmbedding\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "        \n",
    "        # define LLM\n",
    "        self.llm = Ollama(model=\"llama3.2:3b\", request_timeout=5000.0)\n",
    "        self.embed_model=OllamaEmbedding(model_name=\"nomic-embed-text\", request_timeout=5000.0)\n",
    "\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context, embed_model=self.embed_model)\n",
    "        else:\n",
    "            documents = LlamaParse(\n",
    "                result_type=\"markdown\"\n",
    "                , api_key=llama_cloud_api_key\n",
    "                , base_url=llama_cloud_base_url\n",
    "                , content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume_file)\n",
    "\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents\n",
    "                , embed_model=self.embed_model\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "        \n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "    \n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev:ParseFormEvent) -> GenerateQuestionEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key\n",
    "            , base_url=llama_cloud_base_url\n",
    "            , result_type=\"markdown\"\n",
    "            , content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\"\n",
    "            , formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            This is a parsed form.\n",
    "            Convert it into a JSON object containing only the list \n",
    "            of fields to be filled in, in the form {{ fields: [...] }}. \n",
    "            <form>{result.text}</form>.\n",
    "            Return JSON ONLY, no markdown.\"\"\"\n",
    "        )\n",
    "        print(raw_json)\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "        \n",
    "        await ctx.set(\"fields_to_fill\", fields)\n",
    "        return GenerateQuestionEvent()\n",
    "    \n",
    "    @step\n",
    "    async def generate_question(self, ctx: Context, ev: GenerateQuestionEvent | FeedbackEvent) -> QueryEvent:\n",
    "        fields = await ctx.get(\"fields_to_fill\")\n",
    "\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            if hasattr(ev, \"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "            \n",
    "            ctx.send_event(QueryEvent(field=field, query=question))\n",
    "\n",
    "            await ctx.set(\"total_fields\", len(fields))\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "    \n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None \n",
    "        \n",
    "        responseListString = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseListString}\n",
    "            </responses>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"\\nHow does this look? Give me any feedback you have on any of the answers.\"\n",
    "            , result=result\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "        result = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "            \"\"\"\n",
    "        )\n",
    "        print(f\">>>> Raw result verdict: {result}\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if verdict == \"OKAY\":\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 7b3be277-c79f-4637-8a50-856c6ca5938a\n",
      "{\"fields\":[\"First Name\",\"Last Name\",\"Email\",\"Phone\",\"Linkedin\",\"Project Portfolio\",\"Degree\",\"Graduation Date\",\"Current Job title\",\"Technical Skills\",\"Describe why youâ€™re a good fit for this position\",\"Do you have 5 years of experience in React\"]}\n"
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow(verbose=False, timeout=5000)\n",
    "handler = await w.run(\n",
    "    resume_file=\"data/fake_resume.pdf\"\n",
    "    , application_form=\"data/fake_application_form.pdf\"\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(HumanResponseEvent(response=response))\n",
    "\n",
    "response = await handler\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_FILE = \"workflows/form_parsing_workflow.html\"\n",
    "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
    "html_content = extract_html_content(WORKFLOW_FILE)\n",
    "display(HTML(html_content), metadata=dict(isolated=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
